# -*- coding: utf-8 -*-
"""Faraz Streamlit app test 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/gymshark-rollup-5712/locations/us-central1/repositories/0ce77d54-ad13-4375-a690-3092f09b81b9
"""

!pip install sentence-transformers xgboost scikit-learn pandas protobuf==3.20.* streamlit

import os, joblib, numpy as np, pandas as pd, streamlit as st
from sentence_transformers import SentenceTransformer
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# ================== CONFIG ==================
ARTIFACT_DIR = "artifacts"
META_PATH    = f"{ARTIFACT_DIR}/metadata.joblib"
XGB_PATH     = f"{ARTIFACT_DIR}/xgb_model.joblib"
LR_PATH      = f"{ARTIFACT_DIR}/logreg_sent.joblib"
SCALER_PATH  = f"{ARTIFACT_DIR}/sent_scaler.joblib"
META_CLF_PATH= f"{ARTIFACT_DIR}/meta_model.joblib"
EMBED_MODEL_NAME = "all-MiniLM-L12-v2"  # keep in metadata

# ================== STREAMLIT PAGE ==================
st.set_page_config(page_title="Product Quality Predictor", layout="wide")
st.title("Product Quality Predictor")
st.subheader("Check if your product is up to standard")
st.caption("Enter your qualitative feedback to see a predicted rating.")

# ================== DATA LOADING ==================
def load_csv(url):
    # robust loader to avoid dtype warnings
    df = pd.read_csv(url, low_memory=False, encoding="utf-8")
    if "review_text" in df:
        df["review_text"] = df["review_text"].astype(str)
    if "rating" in df:
        df["rating"] = pd.to_numeric(df["rating"], errors="coerce")
    return df

DF_URL = "https://github.com/farazali97/digital-product-colab-test/releases/download/file_upload/reviews.2.csv"
df = load_csv(DF_URL)

# ================== TRAIN & SAVE (one-time) ==================
def train_and_save(df_clean: pd.DataFrame):
    os.makedirs(ARTIFACT_DIR, exist_ok=True)

    # labels
    df_clean = df_clean.dropna(subset=["review_text", "rating"]).copy()
    df_clean["RATING_ZERO_BASED"] = df_clean["rating"].astype(int) - 1
    y = df_clean["RATING_ZERO_BASED"].values

    # embeddings
    embedder = SentenceTransformer(EMBED_MODEL_NAME)
    X_embed = embedder.encode(df_clean["review_text"].tolist(), show_progress_bar=True, batch_size=64)

    # sentiment
    def get_sentiment(txt):
        try:
            b = TextBlob(str(txt))
            return b.sentiment.polarity, b.sentiment.subjectivity
        except:
            return 0.0, 0.0

    df_clean["POLARITY"], df_clean["SUBJECTIVITY"] = zip(*df_clean["review_text"].apply(get_sentiment))
    X_sent = df_clean[["POLARITY", "SUBJECTIVITY"]].values

    # split
    Xtr_e, Xte_e, Xtr_s, Xte_s, ytr, yte = train_test_split(
        X_embed, X_sent, y, test_size=0.05, random_state=42, stratify=y
    )

    # base models
    xgb = XGBClassifier(
        objective="multi:softprob",   # probabilities
        num_class=5,
        eval_metric="mlogloss",
        tree_method="hist"
    )
    xgb.fit(Xtr_e, ytr)

    scaler = StandardScaler()
    Xtr_s_sc = scaler.fit_transform(Xtr_s)

    logreg = LogisticRegression(multi_class="multinomial", max_iter=1000)
    logreg.fit(Xtr_s_sc, ytr)

    # stacking
    p_xgb_tr = xgb.predict_proba(Xtr_e)
    p_lr_tr  = logreg.predict_proba(Xtr_s_sc)
    Z_tr     = np.hstack([p_xgb_tr, p_lr_tr])

    meta = LogisticRegression(multi_class="multinomial", max_iter=1000)
    meta.fit(Z_tr, ytr)

    # save artifacts
    joblib.dump(xgb, XGB_PATH)
    joblib.dump(logreg, LR_PATH)
    joblib.dump(scaler, SCALER_PATH)
    joblib.dump(meta, META_CLF_PATH)
    joblib.dump({"embed_model_name": EMBED_MODEL_NAME, "classes": np.arange(5)}, META_PATH)

# ================== LOAD ARTIFACTS (cached) ==================
@st.cache_resource
def load_artifacts():
    """Load models once per session and cache them in memory."""
    if not all(os.path.exists(p) for p in [META_PATH, XGB_PATH, LR_PATH, SCALER_PATH, META_CLF_PATH]):
        # Train once if missing
        train_and_save(df)

    metadata = joblib.load(META_PATH)
    embedder = SentenceTransformer(metadata["embed_model_name"])
    xgb      = joblib.load(XGB_PATH)
    logreg   = joblib.load(LR_PATH)
    scaler   = joblib.load(SCALER_PATH)
    meta_clf = joblib.load(META_CLF_PATH)
    return embedder, scaler, xgb, logreg, meta_clf, metadata

embedder, scaler, xgb, logreg, meta_clf, metadata = load_artifacts()

# ================== INFERENCE HELPERS ==================
def featurize_single(review_text: str):
    # embedding
    Xe = embedder.encode([review_text], batch_size=1)

    # sentiment
    try:
        b = TextBlob(str(review_text))
        pol, sub = b.sentiment.polarity, b.sentiment.subjectivity
    except:
        pol, sub = 0.0, 0.0
    Xs = np.array([[pol, sub]], dtype=float)
    Xs_sc = scaler.transform(Xs)
    return Xe, Xs_sc

def predict_stars(review_text: str):
    Xe, Xs_sc = featurize_single(review_text)
    p_xgb = xgb.predict_proba(Xe)       # (1,5)
    p_lr  = logreg.predict_proba(Xs_sc) # (1,5)
    Z     = np.hstack([p_xgb, p_lr])    # (1,10)
    probs = meta_clf.predict_proba(Z).ravel()
    stars = int(np.argmax(probs) + 1)   # 1..5
    return stars, probs

# ================== UI ==================
with st.expander("‚öôÔ∏è Input Parameters", expanded=True):
    text_input = st.text_area("Paste a product review:", value="Waistband stayed up during squats; fabric felt durable.")

cols = st.columns([1,1,2])
with cols[0]:
    if st.button("üîÑ Refresh / Retrain Artifacts"):
        # Force retrain and clear cache
        if os.path.exists(ARTIFACT_DIR):
            for f in [META_PATH, XGB_PATH, LR_PATH, SCALER_PATH, META_CLF_PATH]:
                if os.path.exists(f):
                    os.remove(f)
        st.cache_resource.clear()  # clear model cache
        with st.spinner("Training and saving artifacts‚Ä¶"):
            # load_artifacts() will detect missing files and call train_and_save
            embedder, scaler, xgb, logreg, meta_clf, metadata = load_artifacts()
        st.success("Artifacts refreshed.")

with cols[1]:
    run_infer = st.button("üöÄ Predict")

if run_infer:
    if not text_input.strip():
        st.warning("Please paste a review first.")
    else:
        stars, probs = predict_stars(text_input.strip())
        st.success(f"Predicted rating: **{stars}‚òÖ**")
        st.write("Class probabilities (1‚òÖ‚Üí5‚òÖ):", np.round(probs, 3))

st.caption("Tip: Artifacts are cached in memory via `@st.cache_resource`, so the app won‚Äôt retrain on each rerun.")